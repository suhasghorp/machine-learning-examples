{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjoint Method & Backpropagation\n",
    "<div style=\"text-align: right\"> (C) Nikolai Nowaczyk, JÃ¶rg Kienitz 2020</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjoint method is a technique to speed up the calculation of derivatives for complicated functions, which can we written as a decomposition of simpler functions. It is widely used in applied mathematics such as oceanography and mathematical finance. We recap the method and show how it is related to the [backpropagation algorithm of neural networks](https://nbviewer.jupyter.org/github/niknow/machine-learning-examples/blob/master/newton_gradient_backprop/backpropagation.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives and the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key ingredient to the adjoint method is the chain rule.\n",
    "\n",
    "**Theorem (chain rule):** Let $f_1:\\mathbb{R}^{n_0} \\to \\mathbb{R}^{n_1}$ and $f_2:\\mathbb{R}^{n_1} \\to \\mathbb{R}^{n_2}$ be two differentiable funcions and $f := f_2 \\circ f_1: \\mathbb{R}^{n_0} \\to \\mathbb{R}^{n_2}$ be the composition. Then $f$ is differentiable and\n",
    "\\begin{align}\n",
    "    \\forall x \\in \\mathbb{R}^{n_0}: \\nabla f(x) = \\nabla f_2 (f_1(x)) \\bullet \\nabla f_1(x),\n",
    "\\end{align}\n",
    "where $\\bullet$ denotes the ordinary matrix product. \n",
    "\n",
    "Remark: This theorem holds analogously on functions, which are only defined on open subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward method\n",
    "\n",
    "Of course, the chain rule can be applied successively: Assume we are given a scalar function, which is a result of a complicated compisition of functions, i.e. we are given functions $f_l:\\mathbb{R}^{n_{l-1}} \\to \\mathbb{R}^{n_l}$ and $g:\\mathbb{R}^{n_L} \\to \\mathbb{R}$ and we want to calculate the derivative of the composition\n",
    "\\begin{align}\n",
    "    F := g \\circ f_L \\circ \\ldots \\circ f_1: \\mathbb{R}^{n_0} \\to \\mathbb{R}\n",
    "\\end{align}\n",
    "at some point $x_0 \\in \\mathbb{R}^{n_0}$. Then the straight-forward way to calculate this would be to proceed in two steps:\n",
    "\n",
    "1. Starting from $x_0$ successively compute\n",
    "\\begin{align}\n",
    "    x_l := f_l(x_{l-1})\n",
    "\\end{align}\n",
    "for $l=1, \\ldots, L$ and finally  $y := g(x_L)$. \n",
    "\n",
    "2. Compute all the derivatives $\\nabla f_l (x_{l-1})$ for $l=1, \\ldots, L$ and $\\nabla g(x_L)$ and then starting from $D_1 := \\nabla f_1 (x_0)$ sucessively compute\n",
    "\\begin{align}\n",
    "    D_l := \\nabla f_l(x_{l}) \\bullet D_{l-1}\n",
    "\\end{align}\n",
    "and finally $D := \\nabla g (x_L) \\bullet D_L$.\n",
    "\n",
    "Because of the chain rule\n",
    "\\begin{align}\n",
    "    \\nabla F(x_0) = \\nabla g(x_L) \\bullet \\nabla f_L(x_{l-1}) \\bullet \\ldots \\bullet \\nabla f_1(x_0)\n",
    "\\end{align}\n",
    "this *forward method* correctly computes $\\nabla F(x_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "The foward method computes $\\nabla F(x_0)$ by a sequence of matrix multiplications starting on the right of the above equation. That means that in each step when $D_l$ is calculated from $D_{l-1}$ a matrix multiplication needs to be executed, which in a naive implementation has complexity $\\mathcal{O}(n^3)$. \n",
    "\n",
    "**Key insight:** Because the last function $g$ in the chain is scalar valued, the derivative $\\nabla g(x_L)$ is a vector and thus $\\nabla F(x_0) = \\nabla g(x_L) \\bullet D_L$ is also a vector. Thus, computing the matrix products by the foward method just to arrive at a vector is not very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjoint Method\n",
    "The adjoint method computes the $\\nabla F(x_0)$ using the exact same equation of the chain rule but not evaluated from right to left, but from left to right. Mathematically, one often formulates that by computing the adjoint equation instead, which in $\\mathbb{R}^n$ simply amounts to taking the transpose:\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla F(x_0)^{\\top} = \\nabla f_1(x_0)^{\\top} \\bullet \\ldots \\bullet \\nabla f_L(x_{L-1})^{\\top} \\bullet \\nabla g(x_L)^{\\top} \n",
    "\\end{align}\n",
    "\n",
    "Algorithmically, this means that the adjoint method starts by computing $V_{L+1} := \\nabla g(x_L)^{\\top}$ and then computes\n",
    "\n",
    "\\begin{align*}\n",
    "    V_l := \\nabla f_l(x_{l-1})^{\\top} \\bullet V_{l+1}\n",
    "\\end{align*}\n",
    "\n",
    "backwards from $l=L, \\ldots, 1$. \n",
    "\n",
    "This is a sequence of matrix-vector multiplications, which only as complexity $\\mathcal{O}(n^2)$ and is thus much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjoint Method and Backpropagation\n",
    "\n",
    "The forward method and the adjoint method relate to neural networks as follows:\n",
    "\n",
    "Let $\\operatorname{NN} = (A_l, b_l, \\sigma_l)_{1 \\leq l \\leq L}$ be a neural network with $L$ layers. Its feed-forward function can be written as a composition $F = F_L \\circ \\ldots \\circ F_1$ and for each $l$, we have $F_l = \\sigma_l \\circ f_{A_l, b_l}$. Here, for any matrix $A$ and $b$ of compatible dimensions, we denote by $f_{A,b}$ the function $v \\mapsto Av + b$. \n",
    "\n",
    "For any fixed input $x=:a_0$, we can compute the feed-forward via\n",
    "\\begin{align}\n",
    "    z_l := f_{A_l, b_l}(a_{l-1}), && a_l := \\sigma_l(z_l)\n",
    "\\end{align}\n",
    "for $l=1, \\ldots, L$. This is exactly the first step of the forward method above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for that fixed input $x$ also fix a cost function $C$. Then the function $C \\circ F$ is exactly of the shape above, i.e. it is a composition\n",
    "\\begin{align}\n",
    "    C\\circ F &= (C \\circ \\sigma_L) \\circ (f_{A_L, b_L} \\circ \\sigma_{L-1}) \\circ \\ldots \\sigma_2 \\circ (f_{A_2, b_2} \\circ \\sigma_1) \\circ f_{A_1, b_1} \\\\\n",
    "    &=: g \\circ h_L \\circ \\ldots h_2 \\circ f_{A_1, b_1},\n",
    "\\end{align}\n",
    "where $g := C \\circ \\sigma_L$ and $h_l := f_{A_{l+1}, b_{l+1}} \\circ \\sigma_{l}$. Thus, its transposed derivative is given via\n",
    "\\begin{align}\n",
    "    \\nabla (C\\circ F)(x)^{\\top} = \\nabla f_{A_1, b_1}(a_0)^{\\top} \\bullet \\nabla h_2(z_2)^{\\top} \\ldots \\bullet \\nabla h_{L-1}(z_{L-1})^{\\top} \\bullet \\nabla g(z_L)^{\\top}\n",
    "\\end{align}\n",
    "and can be computed via the adjoint method by a backwards sequence of vectors $V_l$, where\n",
    "\\begin{align}\n",
    "    V_{L} & = \\nabla g(z_L)^{\\top} = \\nabla \\sigma_L(z_L)^{\\top} \\bullet \\nabla C(a_L)^{\\top}\n",
    "\\end{align}\n",
    "and for all $l=L-1, \\ldots, 2$\n",
    "\\begin{align}\n",
    "    V_{l} = \\nabla h_l(z_l)^{\\top} \\bullet V_{l+1} \n",
    "    = \\nabla(f_{A_{l+1},b_{l+1}} \\circ \\sigma_{l})(z_l)^{\\top} \\bullet V_{l+1} \n",
    "    = \\nabla \\sigma_{l}(z_l)^{\\top} \\bullet A_{l+1}^{\\top} \\bullet V_{l+1}\n",
    "\\end{align}\n",
    "and finally\n",
    "\\begin{align}\n",
    "    V_1 = \\nabla f_{A_1, b_1}(a_0)^{\\top} \\bullet V_2 = A_1^{\\top} \\bullet V_2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [chapter on backpropagation](https://nbviewer.jupyter.org/github/niknow/machine-learning-examples/blob/master/newton_gradient_backprop/backpropagation.ipynb), we also computed the derivative $\\nabla(C\\circ F)$. There, we defined the functions $G_l := C \\circ F_L \\circ \\ldots \\circ F_{l+1} \\circ \\sigma_l$ and have shown that their gradients $\\varepsilon_l := \\nabla G_l(z_l)$ satisfy \n",
    "\\begin{align*}\n",
    "   \\varepsilon_L &= \\nabla \\sigma_L(z_L)^{\\top} \\bullet \\nabla C(a_L)^{\\top}, \\\\\n",
    "   \\varepsilon_{l} &=  \\nabla \\sigma_{l}(z_l)^{\\top} \\bullet A_{l+1}^{\\top} \\bullet \\varepsilon_{l+1}.\n",
    "\\end{align*}\n",
    "Thus, \n",
    "\\begin{align*}\n",
    "    \\forall 2 \\leq l \\leq L: \\varepsilon_l = V_l,\n",
    "\\end{align*}\n",
    "i.e. the backpropagation lemma is just a special case of the adjoint method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "1. The [Newton's method](https://nbviewer.jupyter.org/github/niknow/machine-learning-examples/blob/master/newton_gradient_backprop/newton.ipynb) finds roots of non-linear functions $F$ and the algorithm needs $\\nabla F$. It can be used for optimization by finding roots of $\\nabla F$. This requires $\\nabla ^2 F$ though, which might not always be conveniently available.\n",
    "\n",
    "2. [Gradien Descent](https://nbviewer.jupyter.org/github/niknow/machine-learning-examples/blob/master/newton_gradient_backprop/gradient_descent.ipynb) minimizes a function $F$ by using only its first order derivative $\\nabla F$.\n",
    "\n",
    "3. [Backpropagation](https://nbviewer.jupyter.org/github/niknow/machine-learning-examples/blob/master/newton_gradient_backprop/backpropagation.ipynb) is a method to efficiently compute the gradient $\\nabla_{\\Theta}(C\\circ F_{\\Theta}(x))$ of the cost function of feed-foward of a neural network with respect to its weights $\\Theta$. This gradient is needed to minimize the cost function via gradient descent. Backpropagation computes this gradient by first computing $\\nabla _x (C \\circ F_{\\Theta})(x)$, the cost of the feed-forward with respect to its input, via a backwards recursion.\n",
    "\n",
    "4. The adjoint method is a general method to compute derivatives of complicated compositions of functions via a backwards recursion. The backpropagation method is a special case of the adjoint method.\n",
    "\n",
    "Notice that as a consequence, we obtain that a neural network whose feed-forward function $F$ has been trained to compute the price of a derivative, also computes the sensitivities of that price using the adjoint method along the way automatically when it runs its backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Giles, Glasserman: Smoking Adjoints\n",
    "* Leclerq, Lian, Schneider: [Fast Monte Carlo Bermudan Greeks](https://www.risk.net/derivatives/interest-rate-derivatives/1500258/fast-monte-carlo-bermudan-greeks), RISK 07/2009\n",
    "* Kienitz, Nowaczyk: [Affine Recursion Problem and a General Framework for Adjoint Methods for Calculating Sensitivities for Financial Instruments](https://ssrn.com/abstract=1957082), SSRN 11/2011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
